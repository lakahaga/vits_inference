tts: tacotron2                   # model architecture
tts_conf:                        # keyword arguments for the selected model
    embed_dim: 512               # char or phn embedding dimension
    elayers: 1                   # number of blstm layers in encoder
    eunits: 512                  # number of blstm units
    econv_layers: 1              # number of convolutional layers in encoder
    econv_chans: 512             # number of channels in convolutional layer
    econv_filts: 5               # filter size of convolutional layer
    atype: forward              # attention function type
    adim: 512                    # attention dimension
    aconv_chans: 32              # number of channels in convolutional layer of attention
    aconv_filts: 15              # filter size of convolutional layer of attention
    cumulate_att_w: true         # whether to cumulate attention weight
    dlayers: 1                   # number of lstm layers in decoder
    dunits: 1024                 # number of lstm units in decoder
    prenet_layers: 1             # number of layers in prenet
    prenet_units: 256            # number of units in prenet
    postnet_layers: 1            # number of layers in postnet
    postnet_chans: 512           # number of channels in postnet
    postnet_filts: 5             # filter size of postnet layer
    output_activation: null      # activation function for the final output
    use_batch_norm: true         # whether to use batch normalization in encoder
    use_concate: true            # whether to concatenate encoder embedding with decoder outputs
    use_residual: false          # whether to use residual connection in encoder
    dropout_rate: 0.0            # dropout rate
    zoneout_rate: 0.1            # zoneout rate
    reduction_factor: 1          # reduction factor
    spk_embed_dim: null          # speaker embedding dimension
    use_masking: true            # whether to apply masking for padded part in loss calculation
    bce_pos_weight: 5.0          # weight of positive sample in binary cross entropy calculation
    use_guided_attn_loss: true   # whether to use guided attention loss
    guided_attn_loss_sigma: 0.4  # sigma of guided attention loss
    guided_attn_loss_lambda: 1.0 # strength of guided attention loss
