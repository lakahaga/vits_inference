lm: transformer
lm_conf:
  att_unit: 256
  dropout_rate: 0.0
  embed_unit: 128
  head: 4
  layer: 3
  pos_enc: sinusoidal
  unit: 256
model_conf:
  ignore_id: 0